{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5281ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short-term memory: thread-based using InMemorySaver\n",
    "# long-term memory: application level using InMemoryStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2b7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory types\n",
    "# semantic - facts\n",
    "# episodic - experiences\n",
    "# procedural - instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1110ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ffa6b",
   "metadata": {},
   "source": [
    "#### Add short term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d4e2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "import uuid\n",
    "\n",
    "checkpoint = InMemorySaver()\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    state[\"messages\"] = \"hello, you call call_model functions\"\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "workflow = (\n",
    "    StateGraph(MessagesState)\n",
    "    .add_node(\"call_model\", call_model)\n",
    "    .add_edge(START, \"call_model\")\n",
    "    .add_edge(\"call_model\", END)\n",
    "    .compile(checkpointer=checkpoint)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57a5324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\":{\n",
    "        \"thread_id\" : str(uuid.uuid4())\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36969fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hello, you call call_model functions\n"
     ]
    }
   ],
   "source": [
    "for chunk in workflow.stream({\"messages\":[{\"role\": \"user\", \"content\": \"what's my name?\"}]}, config=config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f4881",
   "metadata": {},
   "source": [
    "#### subgraph memory management - short memory management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ed148d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "# Subgraph\n",
    "\n",
    "def subgraph_node_1(state: State):\n",
    "    return {\"foo\": state[\"foo\"] + \"bar\"}\n",
    "\n",
    "subgraph_builder = StateGraph(State)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph = subgraph_builder.compile(checkpointer=True) # define internal memory and checkpoint management \n",
    "\n",
    "# Parent graph\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_1\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6d393",
   "metadata": {},
   "source": [
    "#### Read short term memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "029be8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='f7274c10-d8ae-44f0-9f4d-f0808b035fe2'),\n",
       "  HumanMessage(content='hello user12', additional_kwargs={}, response_metadata={}, id='7cdf1dcd-c90e-44a2-942a-f801437fe01b')],\n",
       " 'user_id': 'user12'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpoint = InMemorySaver()\n",
    "\n",
    "class CustomState(MessagesState):\n",
    "    user_id: str\n",
    "\n",
    "def call_model(state: CustomState):\n",
    "    print(state[\"user_id\"])\n",
    "    return {\"messages\": f\"hello {state[\"user_id\"]}\"}\n",
    "\n",
    "workflow = (\n",
    "    StateGraph(CustomState)\n",
    "    .add_node(\"call_model\", call_model)\n",
    "    .add_edge(START, \"call_model\")\n",
    "    .compile(checkpointer=checkpoint)\n",
    ")\n",
    "\n",
    "\n",
    "workflow.invoke({\"messages\":[\"hello\"], \"user_id\": \"user12\"}, config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34808684",
   "metadata": {},
   "source": [
    "#### Write short term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e8c8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_user_info(\n",
    "#     tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "#     config: RunnableConfig\n",
    "# ) -> Command:\n",
    "#     \"\"\"Look up and update user info.\"\"\"\n",
    "#     user_id = config[\"configurable\"].get(\"user_id\")\n",
    "#     name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n",
    "#     return Command(update={\n",
    "#         \"user_name\": name,\n",
    "#         # update the message history\n",
    "#         \"messages\": [\n",
    "#             ToolMessage(\n",
    "#                 \"Successfully looked up user information\",\n",
    "#                 tool_call_id=tool_call_id\n",
    "#             )\n",
    "#         ]\n",
    "#     })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74347cb",
   "metadata": {},
   "source": [
    "#### Add long term memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfafc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use inmemorystore \n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "# use in compile graph\n",
    "# graph.compile(store=store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f37ea",
   "metadata": {},
   "source": [
    "#### Read long term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "691aab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.config import get_store\n",
    "\n",
    "checkpoint = InMemorySaver()\n",
    "store = InMemoryStore()\n",
    "\n",
    "\n",
    "store.put(  \n",
    "    (\"users\",),  \n",
    "    \"1\",  \n",
    "    {\n",
    "        \"name\": \"John Smith\",\n",
    "        \"language\": \"English\",\n",
    "    } \n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"configurable\":{\n",
    "        \"thread_id\": str(uuid.uuid4()),\n",
    "        \"user_id\": \"1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    store = get_store() \n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    user_info = store.get((\"users\",), user_id) \n",
    "    print(user_info)\n",
    "    return {\"messages\": f\"hello, call_model {state[\"messages\"][-1].content}\"}\n",
    "\n",
    "graph = (\n",
    "    StateGraph(MessagesState)\n",
    "    .add_node(\"call_model\", call_model)\n",
    "    .add_edge(START, \"call_model\")\n",
    "    .compile(store=store, checkpointer=checkpoint)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31d7df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item(namespace=['users'], key='1', value={'name': 'John Smith', 'language': 'English'}, created_at='2025-08-05T08:40:37.062542+00:00', updated_at='2025-08-05T08:40:37.062543+00:00')\n",
      "{'call_model': {'messages': 'hello, call_model call fucntion'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream({\"messages\": \"call fucntion\"}, config=config, stream_mode='updates'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae122aa",
   "metadata": {},
   "source": [
    "#### Write long term memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "771c06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from typing import Annotated, Literal\n",
    "from langgraph.config import RunnableConfig, get_store\n",
    "\n",
    "checkpoint = InMemorySaver()\n",
    "store = InMemoryStore()\n",
    "\n",
    "class CustomState(MessagesState):\n",
    "    node_choise: str\n",
    "    save_obj: str\n",
    "\n",
    "def selector(state: CustomState):\n",
    "    print(f\"choise: {state[\"node_choise\"]}\")\n",
    "    return {\"messages\": \"selector is execute\", \"node_choise\": state[\"node_choise\"]}\n",
    "\n",
    "def write_in_memory(state: CustomState, config: RunnableConfig):\n",
    "    store = get_store() \n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    store.put((\"users\",), user_id, state[\"save_obj\"]) \n",
    "    return {\"messages\": \"write in memory\"}\n",
    "\n",
    "def read_from_memory(state: CustomState, config: RunnableConfig):\n",
    "    store = get_store() \n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    user_info = store.get((\"users\",), user_id) \n",
    "    print(user_info)\n",
    "    return {\"messages\": \"read from memory\"}\n",
    "\n",
    "def redirect_node(state: CustomState) -> Literal[\"write_in_memory\", \"read_from_memory\"]:\n",
    "    return state[\"node_choise\"]\n",
    "\n",
    "graph = (\n",
    "    StateGraph(CustomState)\n",
    "    .add_node(\"selector\", selector)\n",
    "    .add_node(\"write_in_memory\", write_in_memory)\n",
    "    .add_node(\"read_from_memory\", read_from_memory)\n",
    "    .add_edge(START, \"selector\")\n",
    "    .add_conditional_edges(\"selector\", redirect_node)\n",
    "    .compile(checkpointer=checkpoint, store=store)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d89c4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\":{\n",
    "        \"thread_id\": str(uuid.uuid4),\n",
    "        \"user_id\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a853803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choise: write_in_memory\n",
      "{'selector': {'messages': 'selector is execute', 'node_choise': 'write_in_memory'}}\n",
      "{'write_in_memory': {'messages': 'write in memory'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream({\"messages\":[\"hello\"], \"node_choise\":\"write_in_memory\", \"save_obj\":\"sample_save\"}, config=config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2eac7226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choise: read_from_memory\n",
      "{'selector': {'messages': 'selector is execute', 'node_choise': 'read_from_memory'}}\n",
      "Item(namespace=['users'], key='1', value='sample_save', created_at='2025-08-05T09:09:49.718009+00:00', updated_at='2025-08-05T09:09:49.718010+00:00')\n",
      "{'read_from_memory': {'messages': 'read from memory'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream({\"messages\":[\"hello\"], \"node_choise\":\"read_from_memory\"}, config=config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88c121",
   "metadata": {},
   "source": [
    "#### Use Semantic Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\", google_api_key=GEMINI_API_KEY)\n",
    "\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 3072,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ad823",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I hate pizza\"})\n",
    "store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber, and i like apple\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa6e54f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['user_123', 'memories'], key='2', value={'text': 'I like apple'}, created_at='2025-08-05T10:01:32.237361+00:00', updated_at='2025-08-05T10:01:32.237365+00:00', score=0.7750916233947487),\n",
       " Item(namespace=['user_123', 'memories'], key='1', value={'text': 'I hate pizza'}, created_at='2025-08-05T10:01:30.989564+00:00', updated_at='2025-08-05T10:01:30.989566+00:00', score=0.7625800174036598)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = store.search(\n",
    "    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=2\n",
    ")\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d76ad",
   "metadata": {},
   "source": [
    "#### Manage short-term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c098c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim messages\n",
    "from langchain_core.messages.utils import (\n",
    "    trim_messages, count_tokens_approximately\n",
    ")\n",
    "\n",
    "def pre_model_processing(state):\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=384,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\")\n",
    "    )\n",
    "\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "# custom strategies - message filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "98947abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete messages\n",
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "# to remove specific message\n",
    "def delete_message(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 2:\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "    \n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "# remove all messages\n",
    "def delete_all_messages(state):\n",
    "    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6712de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize messages\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    # response = llm.invoke(messages)\n",
    "    response = \"demo\"\n",
    "\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "\n",
    "# for agent \n",
    "# from langmem.short_term import SummarizationNode, RunningSummary\n",
    "\n",
    "# class State(AgentState):\n",
    "    # context: dict[str, RunningSummary]\n",
    "\n",
    "# summarization_node = SummarizationNode( \n",
    "#     token_counter=count_tokens_approximately,\n",
    "#     model=model,\n",
    "#     max_tokens=384,\n",
    "#     max_summary_tokens=128,\n",
    "#     output_messages_key=\"llm_input_messages\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4d932486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage checkpoints\n",
    "\n",
    "# func api\n",
    "# graph.get_state(config)\n",
    "# list(graph.get_state_history(config))\n",
    "\n",
    "# checkpoint api\n",
    "# checkpointer.get_tuple(config)\n",
    "# list(checkpointer.list(config))\n",
    "\n",
    "# delete all checkpoint for thread\n",
    "# checkpointer.delete_thread(thread_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f57ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
